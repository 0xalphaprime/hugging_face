{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Tutoral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inferance with Pipelines\n",
    "\n",
    "- a pipeline is a high-level, easy-to-use API for doing inference over a variety of downstream-tasks, including:\n",
    "\n",
    "    - Sequence Classification\n",
    "    - Token Classification (Named Entity Recognition, Part-of-Speech Tagging)\n",
    "    - Question Answering\n",
    "    - Masked Language Modeling\n",
    "    - Feature Extraction\n",
    "\n",
    "- The pipeline will take care of:\n",
    "- Loading the model\n",
    "- Tokenizing the input\n",
    "- Running inference\n",
    "- Post-processing\n",
    "- Outputing the result in a structured object\n",
    "- The pipeline API is task-agnostic and can be used for either CPU or GPU inference.\n",
    "\n",
    "\n",
    "**REMEMBER TO RESTART YOUR KERNEL AFTER INSTALLING NEW LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2691, 0.3061, 0.3476],\n",
      "        [0.8826, 0.7279, 0.0232],\n",
      "        [0.9664, 0.6160, 0.1433],\n",
      "        [0.3304, 0.6357, 0.2390],\n",
      "        [0.5368, 0.4442, 0.0866]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ab314788c4d648c9758b408e70060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b8a2ec4e6d445a9dcebe4ad574375f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d734cfe3a4f4057b331ad98b81e6b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129f0bdff6134895a21d2f7df6e80fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# sentiment-analysis is only one of the many pipelines available in the library\n",
    "# other piplines include: \n",
    "# feature-extraction, fill-mask, ner, question-answering, summarization, text-generation, translation, zero-shot-classification\n",
    "# see https://huggingface.co/transformers/main_classes/pipelines.html for more details\n",
    "\n",
    "res = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course we will teach you how to use one another while on a daily basis. We are not talking about technical, functional, nor about technical'}, {'generated_text': 'In this course we will teach you how to take advantage of these incredible resources in high school. Through classes you will learn how to take advantage of these'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "res = generator(\n",
    "    'In this course we will teach you how to',\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about Python list comprehension', 'labels': ['education', 'business', 'politics'], 'scores': [0.9622018337249756, 0.02684188075363636, 0.010956265963613987]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "res = classifier(\n",
    "    \"This is a course about Python list comprehension\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 378M/378M [00:11<00:00, 31.7MB/s] \n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"automatic-speech-recognition\")\n",
    "\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.99k/1.99k [00:00<00:00, 20.8MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 6.17G/6.17G [03:24<00:00, 30.1MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 4.26k/4.26k [00:00<00:00, 31.6MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 800/800 [00:00<00:00, 12.7MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 836k/836k [00:00<00:00, 14.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.20M/2.20M [00:00<00:00, 12.3MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 494k/494k [00:00<00:00, 35.9MB/s]\n",
      "Downloading (…)main/normalizer.json: 100%|██████████| 52.7k/52.7k [00:00<00:00, 67.2MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.08k/2.08k [00:00<00:00, 7.06MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.08k/2.08k [00:00<00:00, 11.0MB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 185k/185k [00:00<00:00, 38.6MB/s]\n",
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use diffferent models \n",
    "# most popualr ar listed at https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='openai/whisper-large-v2')\n",
    "\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.53k/1.53k [00:00<00:00, 19.6MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.26G/1.26G [00:36<00:00, 34.2MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 300/300 [00:00<00:00, 4.61MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 1.06MB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 262/262 [00:00<00:00, 1.45MB/s]\n",
      "Could not load the `decoder` for jonatasgrosman/wav2vec2-large-xlsr-53-english. Defaulting to raw CTC. Error: No module named 'kenlm'\n",
      "Try to install `kenlm`: `pip install kenlm\n",
      "Try to install `pyctcdecode`: `pip install pyctcdecode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'i have a dreambut one daythis nation will rise up and live up the true meaning of its creed'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this model was number 1 first time running through the tutorial\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='jonatasgrosman/wav2vec2-large-xlsr-53-english')\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.38k/1.38k [00:00<00:00, 18.3MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.26G/1.26G [00:38<00:00, 32.9MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 138/138 [00:00<00:00, 2.23MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 291/291 [00:00<00:00, 1.84MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 519kB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 212/212 [00:00<00:00, 1.34MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I HAVE A DREAM THAT ONE DAY THIS NATION WILL RISE UP AND LIVE OUT THE TRUE MEANING OF ITS CREED'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# facebook hubert model\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='facebook/hubert-large-ls960-ft')\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'I HAVE A DREAM THAT ONE DAY THIS NATION WILL RISE UP AND LIVE OUT THE TRUE MEANING OF ITS CREED'},\n",
       " {'text': 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple inputs\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='facebook/hubert-large-ls960-ft')\n",
    "\n",
    "generator(\n",
    "    [\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\",\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but your input_length is only 255. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' RPF (vs WC) was associated with an increase in sternal healing scores of 0.68 (95% confidence interval [95% CI], 0.41-0.95) at 3 months and 0.47 at 6 months. Factors independently associated with reduced sternal. healing at both 3 and 6 months included increased age (per. 10 years), increasing BMI (per 5 kg/m2), and current smoker (Table. 2) RPF was also associated with greater rates of sternal union at 3. months (41% [42/103] vs 16% [16/102]; P < .0001) than WC (16% vs. 1.8%) (Table 2) Compared with WC, RPF resulted in better sternal Healing scores at 3 (2.6 ± 1.1 vs 1.7 ± 1,0; P = .0007) and 6 (3.8±1.0 vs 3.3±1,0) months (Table 1) compared with WC (1.8/1, 1.3/1.1) (Table 3) Compared to WC (vs. RPF), RPF had better rates of. sternalunion at 3/3 months and 6/6 months (80% [81/101] vs 67% [67/100]; P =.03)'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model='facebook/bart-large-cnn')\n",
    "\n",
    "ARTICLE = \"\"\"\n",
    "Sternal Healing\n",
    "Compared with WC, RPF resulted in better sternal healing scores at 3 (2.6 ± 1.1 vs 1.8 ± 1.0; P < .0001) and 6 months (3.8 ± 1.0 vs 3.3 ± 1.1; P = .0007) while also achieving greater rates of sternal union at 3 months (41% [42/103] vs 16% [16/102]; P < .0001) and 6 months (80% [81/101] vs 67% [67/100]; P = .03).\n",
    "\n",
    "Based on multiple linear regression, factors independently associated with reduced sternal healing at both 3 and 6 months included increased age (per 10 years), increasing BMI (per 5 kg/m2), and current smoker (Table 2). After we adjusted for these factors, RPF (vs WC) was associated with an increase in sternal healing scores of 0.68 (95% confidence interval [95% CI], 0.41-0.95; P < .0001) at 3 months and 0.47 (95% CI, 0.19-0.75; P = .001) at 6 months.\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(ARTICLE, max_length=300, min_length=250, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 69.7k/69.7k [00:00<00:00, 5.74MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 346M/346M [00:12<00:00, 26.8MB/s] \n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 160/160 [00:00<00:00, 2.24MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4403, 'label': 'lynx, catamount'},\n",
       " {'score': 0.0343,\n",
       "  'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'},\n",
       " {'score': 0.0321, 'label': 'snow leopard, ounce, Panthera uncia'},\n",
       " {'score': 0.0235, 'label': 'Egyptian cat'},\n",
       " {'score': 0.023, 'label': 'tiger cat'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualiztion tasks \n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "preds = vision_classifier(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n",
    "\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.994, 'label': 'lion, king of beasts, Panthera leo'},\n",
       " {'score': 0.002, 'label': 'leopard, Panthera pardus'},\n",
       " {'score': 0.0009,\n",
       "  'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'},\n",
       " {'score': 0.0008, 'label': 'cheetah, chetah, Acinonyx jubatus'},\n",
       " {'score': 0.0005, 'label': 'jaguar, panther, Panthera onca, Felis onca'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "preds = vision_classifier(\"https://images.pexels.com/photos/4003489/pexels-photo-4003489.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\")\n",
    "\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [00:55<00:00, 29.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5036359429359436,\n",
       "  0.478799045085907,\n",
       "  0.012600546702742577,\n",
       "  0.00265577994287014,\n",
       "  0.002308762166649103]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# This model is a `zero-shot-classification` model.\n",
    "# It will classify text, except you are free to choose any label you might imagine\n",
    "classifier = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6504452228546143,\n",
       "  'answer': 'Total_[s11981.00',\n",
       "  'start': 132,\n",
       "  'end': 132}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal pipeline\n",
    "\n",
    "# visual question answering (VQA) task combines text and image. Feel free to use any image \n",
    "# link you like and a question you want to ask about the image. The image can be a \n",
    "# URL or a local path to the image.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "vqa = pipeline(model=\"impira/layoutlm-document-qa\")\n",
    "vqa(\n",
    "    image=\"/Users/alphaprime/programming/hugging_face/static/img/screenshot_delray_DO.png\",\n",
    "    question=\"What is the invoice total dollar cost of the invoice?\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/sacremoses-0.0.43-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/sacremoses-0.0.43-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (4.29.2)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/83/8d/f65f8138365462ace54458a9e164f4b28ce1141361970190eef36bdef986/transformers-4.32.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/34/0e/12d55d5dd648b8f7ea7216c5b7cef9703b4dbd3b2a042872c711d5e98551/safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl (406 kB)\n",
      "Installing collected packages: safetensors, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed safetensors-0.3.3 transformers-4.32.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate && pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"This is a cool example of the way the internet was used in the '90s as a means\"}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", device=-1)\n",
    "output = pipe(\"This is a cool example\", do_sample=True, top_p=0.95)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 4ff94451-7ad8-4e4e-973d-52348f8df0d3)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 3.26MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 382kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 8.95MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 4.86MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1252,  1184,  1164,  1248,  6462,   136,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1790,   112,   189,  1341,  1119,  3520,  1164,  1248,  6462,\n",
      "           117, 21902,  1643,   119,   102],\n",
      "        [  101,  1327,  1164,  5450, 23434,   136,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# padding - not all senences are the same length, so we need to pad the shorter ones\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/sacremoses-0.0.43-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: soundfile in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 159/159 [00:00<00:00, 410kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.84k/1.84k [00:00<00:00, 25.1MB/s]\n",
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n",
       "       -2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "\n",
    "ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "audio_input = [ds[0][\"audio\"][\"array\"]]\n",
    "\n",
    "feature_extractor(audio_input, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to preprocess the dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        max_length=100000,\n",
    "        truncation=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': [array([ 8.7965636e-05,  3.2919589e-03, -3.1160279e-03, ...,\n",
      "        8.7965636e-05,  8.7965636e-05,  8.7965636e-05], dtype=float32), array([0.00039417, 0.00901931, 0.00039417, ..., 0.00039417, 0.00039417,\n",
      "       0.00039417], dtype=float32), array([ 2.9196806e-04,  2.9196806e-04,  2.9196806e-04, ...,\n",
      "       -1.8273288e+00, -1.8273288e+00, -1.7310591e+00], dtype=float32), array([ 4.3811044e-03, -9.7036864e-06, -9.7036864e-06, ...,\n",
      "       -9.7036864e-06, -9.7036864e-06, -9.7036864e-06], dtype=float32), array([-3.244290e-03,  9.256339e-06, -3.244290e-03, ...,  9.256339e-06,\n",
      "        9.256339e-06,  9.256339e-06], dtype=float32)]}\n"
     ]
    }
   ],
   "source": [
    "# apply the function to the dataset (ds)\n",
    "\n",
    "preprocessed_ds = preprocess_function(ds[:5])\n",
    "print(preprocessed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer vision\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "dataset = load_dataset(\"food101\", split=\"train[:100]\")\n",
    "\n",
    "dataset[0][\"image\"]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "se state-of-the-art models without having to train one from scratch. 🤗 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# continuing with its own file for fine-tuning... see this directory for more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
