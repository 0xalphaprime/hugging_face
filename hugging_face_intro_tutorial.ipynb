{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Tutoral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inferance with Pipelines\n",
    "\n",
    "- a pipeline is a high-level, easy-to-use API for doing inference over a variety of downstream-tasks, including:\n",
    "\n",
    "    - Sequence Classification\n",
    "    - Token Classification (Named Entity Recognition, Part-of-Speech Tagging)\n",
    "    - Question Answering\n",
    "    - Masked Language Modeling\n",
    "    - Feature Extraction\n",
    "\n",
    "- The pipeline will take care of:\n",
    "- Loading the model\n",
    "- Tokenizing the input\n",
    "- Running inference\n",
    "- Post-processing\n",
    "- Outputing the result in a structured object\n",
    "- The pipeline API is task-agnostic and can be used for either CPU or GPU inference.\n",
    "\n",
    "\n",
    "**REMEMBER TO RESTART YOUR KERNEL AFTER INSTALLING NEW LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2691, 0.3061, 0.3476],\n",
      "        [0.8826, 0.7279, 0.0232],\n",
      "        [0.9664, 0.6160, 0.1433],\n",
      "        [0.3304, 0.6357, 0.2390],\n",
      "        [0.5368, 0.4442, 0.0866]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ab314788c4d648c9758b408e70060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b8a2ec4e6d445a9dcebe4ad574375f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d734cfe3a4f4057b331ad98b81e6b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129f0bdff6134895a21d2f7df6e80fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# sentiment-analysis is only one of the many pipelines available in the library\n",
    "# other piplines include: \n",
    "# feature-extraction, fill-mask, ner, question-answering, summarization, text-generation, translation, zero-shot-classification\n",
    "# see https://huggingface.co/transformers/main_classes/pipelines.html for more details\n",
    "\n",
    "res = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course we will teach you how to use one another while on a daily basis. We are not talking about technical, functional, nor about technical'}, {'generated_text': 'In this course we will teach you how to take advantage of these incredible resources in high school. Through classes you will learn how to take advantage of these'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "res = generator(\n",
    "    'In this course we will teach you how to',\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about Python list comprehension', 'labels': ['education', 'business', 'politics'], 'scores': [0.9622018337249756, 0.02684188075363636, 0.010956265963613987]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "res = classifier(\n",
    "    \"This is a course about Python list comprehension\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 378M/378M [00:11<00:00, 31.7MB/s] \n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"automatic-speech-recognition\")\n",
    "\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.99k/1.99k [00:00<00:00, 20.8MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 6.17G/6.17G [03:24<00:00, 30.1MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 4.26k/4.26k [00:00<00:00, 31.6MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 800/800 [00:00<00:00, 12.7MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 836k/836k [00:00<00:00, 14.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.20M/2.20M [00:00<00:00, 12.3MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 494k/494k [00:00<00:00, 35.9MB/s]\n",
      "Downloading (…)main/normalizer.json: 100%|██████████| 52.7k/52.7k [00:00<00:00, 67.2MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.08k/2.08k [00:00<00:00, 7.06MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.08k/2.08k [00:00<00:00, 11.0MB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 185k/185k [00:00<00:00, 38.6MB/s]\n",
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use diffferent models \n",
    "# most popualr ar listed at https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='openai/whisper-large-v2')\n",
    "\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.53k/1.53k [00:00<00:00, 19.6MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.26G/1.26G [00:36<00:00, 34.2MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 300/300 [00:00<00:00, 4.61MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 1.06MB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 262/262 [00:00<00:00, 1.45MB/s]\n",
      "Could not load the `decoder` for jonatasgrosman/wav2vec2-large-xlsr-53-english. Defaulting to raw CTC. Error: No module named 'kenlm'\n",
      "Try to install `kenlm`: `pip install kenlm\n",
      "Try to install `pyctcdecode`: `pip install pyctcdecode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'i have a dreambut one daythis nation will rise up and live up the true meaning of its creed'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this model was number 1 first time running through the tutorial\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='jonatasgrosman/wav2vec2-large-xlsr-53-english')\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.38k/1.38k [00:00<00:00, 18.3MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.26G/1.26G [00:38<00:00, 32.9MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 138/138 [00:00<00:00, 2.23MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 291/291 [00:00<00:00, 1.84MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 519kB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 212/212 [00:00<00:00, 1.34MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I HAVE A DREAM THAT ONE DAY THIS NATION WILL RISE UP AND LIVE OUT THE TRUE MEANING OF ITS CREED'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# facebook hubert model\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='facebook/hubert-large-ls960-ft')\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'I HAVE A DREAM THAT ONE DAY THIS NATION WILL RISE UP AND LIVE OUT THE TRUE MEANING OF ITS CREED'},\n",
       " {'text': 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple inputs\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model='facebook/hubert-large-ls960-ft')\n",
    "\n",
    "generator(\n",
    "    [\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\",\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but your input_length is only 255. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' RPF (vs WC) was associated with an increase in sternal healing scores of 0.68 (95% confidence interval [95% CI], 0.41-0.95) at 3 months and 0.47 at 6 months. Factors independently associated with reduced sternal. healing at both 3 and 6 months included increased age (per. 10 years), increasing BMI (per 5 kg/m2), and current smoker (Table. 2) RPF was also associated with greater rates of sternal union at 3. months (41% [42/103] vs 16% [16/102]; P < .0001) than WC (16% vs. 1.8%) (Table 2) Compared with WC, RPF resulted in better sternal Healing scores at 3 (2.6 ± 1.1 vs 1.7 ± 1,0; P = .0007) and 6 (3.8±1.0 vs 3.3±1,0) months (Table 1) compared with WC (1.8/1, 1.3/1.1) (Table 3) Compared to WC (vs. RPF), RPF had better rates of. sternalunion at 3/3 months and 6/6 months (80% [81/101] vs 67% [67/100]; P =.03)'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model='facebook/bart-large-cnn')\n",
    "\n",
    "ARTICLE = \"\"\"\n",
    "Sternal Healing\n",
    "Compared with WC, RPF resulted in better sternal healing scores at 3 (2.6 ± 1.1 vs 1.8 ± 1.0; P < .0001) and 6 months (3.8 ± 1.0 vs 3.3 ± 1.1; P = .0007) while also achieving greater rates of sternal union at 3 months (41% [42/103] vs 16% [16/102]; P < .0001) and 6 months (80% [81/101] vs 67% [67/100]; P = .03).\n",
    "\n",
    "Based on multiple linear regression, factors independently associated with reduced sternal healing at both 3 and 6 months included increased age (per 10 years), increasing BMI (per 5 kg/m2), and current smoker (Table 2). After we adjusted for these factors, RPF (vs WC) was associated with an increase in sternal healing scores of 0.68 (95% confidence interval [95% CI], 0.41-0.95; P < .0001) at 3 months and 0.47 (95% CI, 0.19-0.75; P = .001) at 6 months.\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(ARTICLE, max_length=300, min_length=250, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 69.7k/69.7k [00:00<00:00, 5.74MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 346M/346M [00:12<00:00, 26.8MB/s] \n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 160/160 [00:00<00:00, 2.24MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4403, 'label': 'lynx, catamount'},\n",
       " {'score': 0.0343,\n",
       "  'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'},\n",
       " {'score': 0.0321, 'label': 'snow leopard, ounce, Panthera uncia'},\n",
       " {'score': 0.0235, 'label': 'Egyptian cat'},\n",
       " {'score': 0.023, 'label': 'tiger cat'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualiztion tasks \n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "preds = vision_classifier(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n",
    "\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.994, 'label': 'lion, king of beasts, Panthera leo'},\n",
       " {'score': 0.002, 'label': 'leopard, Panthera pardus'},\n",
       " {'score': 0.0009,\n",
       "  'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'},\n",
       " {'score': 0.0008, 'label': 'cheetah, chetah, Acinonyx jubatus'},\n",
       " {'score': 0.0005, 'label': 'jaguar, panther, Panthera onca, Felis onca'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "preds = vision_classifier(\"https://images.pexels.com/photos/4003489/pexels-photo-4003489.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\")\n",
    "\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [00:55<00:00, 29.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5036359429359436,\n",
       "  0.478799045085907,\n",
       "  0.012600546702742577,\n",
       "  0.00265577994287014,\n",
       "  0.002308762166649103]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# This model is a `zero-shot-classification` model.\n",
    "# It will classify text, except you are free to choose any label you might imagine\n",
    "classifier = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6504452228546143,\n",
       "  'answer': 'Total_[s11981.00',\n",
       "  'start': 132,\n",
       "  'end': 132}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal pipeline\n",
    "\n",
    "# visual question answering (VQA) task combines text and image. Feel free to use any image \n",
    "# link you like and a question you want to ask about the image. The image can be a \n",
    "# URL or a local path to the image.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "vqa = pipeline(model=\"impira/layoutlm-document-qa\")\n",
    "vqa(\n",
    "    image=\"/Users/alphaprime/programming/hugging_face/static/img/screenshot_delray_DO.png\",\n",
    "    question=\"What is the invoice total dollar cost of the invoice?\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/sacremoses-0.0.43-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/sacremoses-0.0.43-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (4.29.2)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/83/8d/f65f8138365462ace54458a9e164f4b28ce1141361970190eef36bdef986/transformers-4.32.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/34/0e/12d55d5dd648b8f7ea7216c5b7cef9703b4dbd3b2a042872c711d5e98551/safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl (406 kB)\n",
      "Installing collected packages: safetensors, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed safetensors-0.3.3 transformers-4.32.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate && pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"This is a cool example of the way the internet was used in the '90s as a means\"}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", device=-1)\n",
    "output = pipe(\"This is a cool example\", do_sample=True, top_p=0.95)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 4ff94451-7ad8-4e4e-973d-52348f8df0d3)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 3.26MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 382kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 8.95MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 4.86MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1252,  1184,  1164,  1248,  6462,   136,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1790,   112,   189,  1341,  1119,  3520,  1164,  1248,  6462,\n",
      "           117, 21902,  1643,   119,   102],\n",
      "        [  101,  1327,  1164,  5450, 23434,   136,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# padding - not all senences are the same length, so we need to pad the shorter ones\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/sacremoses-0.0.43-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/huggingface_hub-0.16.4-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: soundfile in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 159/159 [00:00<00:00, 410kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.84k/1.84k [00:00<00:00, 25.1MB/s]\n",
      "/Users/alphaprime/miniconda3/envs/hugging_face/lib/python3.11/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n",
       "       -2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "\n",
    "ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "audio_input = [ds[0][\"audio\"][\"array\"]]\n",
    "\n",
    "feature_extractor(audio_input, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to preprocess the dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        max_length=100000,\n",
    "        truncation=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': [array([ 8.7965636e-05,  3.2919589e-03, -3.1160279e-03, ...,\n",
      "        8.7965636e-05,  8.7965636e-05,  8.7965636e-05], dtype=float32), array([0.00039417, 0.00901931, 0.00039417, ..., 0.00039417, 0.00039417,\n",
      "       0.00039417], dtype=float32), array([ 2.9196806e-04,  2.9196806e-04,  2.9196806e-04, ...,\n",
      "       -1.8273288e+00, -1.8273288e+00, -1.7310591e+00], dtype=float32), array([ 4.3811044e-03, -9.7036864e-06, -9.7036864e-06, ...,\n",
      "       -9.7036864e-06, -9.7036864e-06, -9.7036864e-06], dtype=float32), array([-3.244290e-03,  9.256339e-06, -3.244290e-03, ...,  9.256339e-06,\n",
      "        9.256339e-06,  9.256339e-06], dtype=float32)]}\n"
     ]
    }
   ],
   "source": [
    "# apply the function to the dataset (ds)\n",
    "\n",
    "preprocessed_ds = preprocess_function(ds[:5])\n",
    "print(preprocessed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.21k/6.21k [00:00<00:00, 17.0MB/s]\n",
      "Downloading metadata: 100%|██████████| 5.56k/5.56k [00:00<00:00, 32.1MB/s]\n",
      "Downloading readme: 100%|██████████| 10.3k/10.3k [00:00<00:00, 36.5MB/s]\n",
      "Downloading data:  45%|████▍     | 2.24G/5.00G [15:10<18:38, 2.46MB/s]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoImageProcessor\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mfood101\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain[:100]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m dataset[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m image_processor \u001b[39m=\u001b[39m AutoImageProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/vit-base-patch16-224\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/load.py:2136\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2133\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2135\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2136\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   2137\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2138\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2139\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   2140\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   2141\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   2142\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2143\u001b[0m )\n\u001b[1;32m   2145\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   2147\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   2148\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/builder.py:954\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 954\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    955\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    956\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    957\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    958\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    959\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/builder.py:1717\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1717\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1718\u001b[0m         dl_manager,\n\u001b[1;32m   1719\u001b[0m         verification_mode,\n\u001b[1;32m   1720\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1721\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1722\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1723\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name)\n\u001b[1;32m   1026\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m-> 1027\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msplit_generators_kwargs)\n\u001b[1;32m   1029\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/food101/7cebe41a80fb2da3f08fcbef769c8874073a86346f7fb96dc0847d4dfc318295/food101.py:187\u001b[0m, in \u001b[0;36mFood101._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[0;32m--> 187\u001b[0m     archive_path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload(_BASE_URL)\n\u001b[1;32m    188\u001b[0m     split_metadata_paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownload(_METADATA_URLS)\n\u001b[1;32m    189\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    190\u001b[0m         datasets\u001b[39m.\u001b[39mSplitGenerator(\n\u001b[1;32m    191\u001b[0m             name\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m         ),\n\u001b[1;32m    204\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/download/download_manager.py:427\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    424\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[1;32m    426\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 427\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m    428\u001b[0m     download_func,\n\u001b[1;32m    429\u001b[0m     url_or_urls,\n\u001b[1;32m    430\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    431\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m    432\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[1;32m    433\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    434\u001b[0m )\n\u001b[1;32m    435\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    436\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds()\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/utils/py_utils.py:455\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m# Singleton\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    457\u001b[0m disable_tqdm \u001b[39m=\u001b[39m disable_tqdm \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m    458\u001b[0m iterable \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data_struct\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m data_struct\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/download/download_manager.py:453\u001b[0m, in \u001b[0;36mDownloadManager._download\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    451\u001b[0m     \u001b[39m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     url_or_filename \u001b[39m=\u001b[39m url_or_path_join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(url_or_filename, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/utils/file_utils.py:182\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    181\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    183\u001b[0m         url_or_filename,\n\u001b[1;32m    184\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    185\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    186\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    187\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    188\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    189\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    190\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    191\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    192\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m    193\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    194\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    195\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    198\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/utils/file_utils.py:644\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    642\u001b[0m         fsspec_get(url, temp_file, storage_options\u001b[39m=\u001b[39mstorage_options, desc\u001b[39m=\u001b[39mdownload_desc)\n\u001b[1;32m    643\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m         http_get(\n\u001b[1;32m    645\u001b[0m             url,\n\u001b[1;32m    646\u001b[0m             temp_file,\n\u001b[1;32m    647\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    648\u001b[0m             resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    649\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    650\u001b[0m             cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    651\u001b[0m             max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    652\u001b[0m             desc\u001b[39m=\u001b[39;49mdownload_desc,\n\u001b[1;32m    653\u001b[0m         )\n\u001b[1;32m    655\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    656\u001b[0m shutil\u001b[39m.\u001b[39mmove(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/datasets/utils/file_utils.py:419\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[1;32m    410\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m    412\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    413\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m    418\u001b[0m ) \u001b[39mas\u001b[39;00m progress:\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    420\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n\u001b[1;32m    421\u001b[0m         temp_file\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging_face/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# computer vision\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "dataset = load_dataset(\"food101\", split=\"train[:100]\")\n",
    "\n",
    "dataset[0][\"image\"]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "se state-of-the-art models without having to train one from scratch. 🤗 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# continuing with its own file for fine-tuning... see this directory for more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
